import pandas as pd
from crawler import Crawler
import logging
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def main():
    # Banner khá»Ÿi Ä‘á»™ng
    logger.info("=" * 100)
    logger.info("ğŸš€ KHá»I Äá»˜NG CHÆ¯Æ NG TRÃŒNH CRAWL KIá»‚U DÃNG CÃ”NG NGHIá»†P - NOIP VIETNAM")
    logger.info("=" * 100)

    driver_path = "chromedriver-win64/chromedriver.exe"
    excel_path = "data_kdcn_bo_sung.xlsx"
    restart_interval = 100  # Khá»Ÿi Ä‘á»™ng láº¡i driver sau má»—i 100 láº§n tÃ¬m kiáº¿m

    logger.info(f"âš™ï¸  Cáº¤U HÃŒNH:")
    logger.info(f"   â€¢ ChromeDriver: {driver_path}")
    logger.info(f"   â€¢ File input: {excel_path}")
    logger.info(f"   â€¢ Restart interval: {restart_interval} láº§n tÃ¬m kiáº¿m")
    logger.info(f"   â€¢ Loáº¡i crawl: DESIGNS (Kiá»ƒu dÃ¡ng cÃ´ng nghiá»‡p)")
    logger.info("")
    logger.info(f"ğŸ“‚ Cáº¤U TRÃšC THÆ¯ Má»¤C OUTPUT:")
    logger.info(f"   Output_Designs/")
    logger.info(f"   â”œâ”€â”€ designs_data.xlsx          (File Excel chá»©a dá»¯ liá»‡u)")
    logger.info(f"   â”œâ”€â”€ Images/                    (ThÆ° má»¥c áº£nh kiá»ƒu dÃ¡ng)")
    logger.info(f"   â”‚   â”œâ”€â”€ [Sá»‘ Ä‘Æ¡n 1]/")
    logger.info(f"   â”‚   â”œâ”€â”€ [Sá»‘ Ä‘Æ¡n 2]/")
    logger.info(f"   â”‚   â””â”€â”€ ...")
    logger.info(f"   â””â”€â”€ Errors/                    (Screenshot lá»—i - náº¿u cÃ³)")
    logger.info(f"       â”œâ”€â”€ phase_1_exception/")
    logger.info(f"       â”œâ”€â”€ phase_2_timeout/")
    logger.info(f"       â””â”€â”€ phase_3_other/")
    logger.info("")

    crawler = Crawler(driver_path, excel_path, restart_interval)

    try:
        sheet_name = 0
        data = pd.read_excel(excel_path, sheet_name=sheet_name)
        column_name = "filing_number"  # Äáº£m báº£o Ä‘Ã¢y lÃ  tÃªn cá»™t chÃ­nh xÃ¡c

        logger.info(f"ğŸ“– Äá»ŒC Dá»® LIá»†U INPUT:")
        logger.info(f"   â€¢ Sheet: {sheet_name}")
        logger.info(f"   â€¢ Cá»™t sá»‘ Ä‘Æ¡n: {column_name}")
        logger.info(f"   â€¢ Tá»•ng sá»‘ dÃ²ng trong file: {len(data)}")

        start_index = 0
        if crawler.last_so_don:
            start_index = data[data[column_name] == crawler.last_so_don].index[0] + 1
            logger.info(f"   â€¢ Tiáº¿p tá»¥c tá»« sá»‘ Ä‘Æ¡n: {crawler.last_so_don}")
            logger.info(f"   â€¢ Báº¯t Ä‘áº§u tá»« dÃ²ng: {start_index + 1}")

        total_searches = len(data) - start_index
        logger.info(f"   â€¢ Sá»‘ Ä‘Æ¡n cáº§n crawl: {total_searches}")
        logger.info("")
        logger.info("=" * 100)
        logger.info("ğŸ¯ Báº®T Äáº¦U CRAWL Dá»® LIá»†U")
        logger.info("=" * 100)
        logger.info("")

        with tqdm(
            total=total_searches, desc="â³ Tiáº¿n trÃ¬nh crawl", unit=" Ä‘Æ¡n"
        ) as pbar:
            for index in range(start_index, len(data)):
                row = data.iloc[index]
                search_value = row[column_name]
                logger.info(f"ğŸ“Œ ÄÆ¡n {index + 1 - start_index}/{total_searches}")
                crawler.process_search(search_value)
                pbar.update(1)

    finally:
        logger.info("")
        logger.info("=" * 100)
        logger.info("ğŸ ÄÃ“NG TRÃŒNH DUYá»†T")
        logger.info("=" * 100)
        crawler.close_driver()

    logger.info("")
    logger.info("=" * 100)
    logger.info("âœ… HOÃ€N Táº¤T! QuÃ¡ trÃ¬nh crawl Ä‘Ã£ káº¿t thÃºc.")
    logger.info(f"ğŸ“ KIá»‚M TRA Káº¾T QUáº¢ Táº I THá»¦ Má»¤C: Output_Designs/")
    logger.info(f"   â€¢ File Excel dá»¯ liá»‡u: Output_Designs/designs_data.xlsx")
    logger.info(f"   â€¢ ThÆ° má»¥c áº£nh kiá»ƒu dÃ¡ng: Output_Designs/Images/")
    logger.info(f"   â€¢ Screenshot lá»—i (náº¿u cÃ³): Output_Designs/Errors/")
    logger.info(f"     - phase_1_exception: Lá»—i exception chung")
    logger.info(f"     - phase_2_timeout: Lá»—i timeout khÃ´ng tÃ¬m tháº¥y")
    logger.info(f"     - phase_3_other: Lá»—i khÃ¡c")
    logger.info("=" * 100)


if __name__ == "__main__":
    main()
